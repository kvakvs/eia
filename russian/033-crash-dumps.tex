%\chapter{Reading Crash Dumps}
\chapter{Читаем файлы аварийных дампов}
\label{chap:crash-dumps}

%Whenever an Erlang node crashes, it will generate a crash dump\footnote{If it isn't killed by the OS for violating ulimits while dumping or didn't segfault.}.
Когда Erlang-узел завершает работу аварийно, он создаёт файл аварийного дампа\footnote{Если он не нарушил ограничения ОС (ulimit) во время записи файла или не упал с ошибкой сегментации (segfault).}.

%The format is mostly documented in Erlang's official documentation\footnote{\href{http://www.erlang.org/doc/apps/erts/crash\_dump.html}{http://www.erlang.org/doc/apps/erts/crash\_dump.html}}, and anyone willing to dig deeper inside of it will likely be able to figure out what data means by looking at that documentation. There will be specific data that is hard to understand without also understanding the part of the VM they refer to, but that might be too complex for this document.
Формат такого файла в основном описан в официальной документации\footnote{\href{http://www.erlang.org/doc/apps/erts/crash\_dump.html}{http://www.erlang.org/doc/apps/erts/crash\_dump.html}}, и любой, кто желает копнуть поглубже, вероятно сможет разобраться, что означают данные, глядя в документацию. Есть некоторые значения, которые особенно трудно понять, если не понимать ту часть виртуальной машины, на которую они ссылаются, но для этого документа такое описание может оказаться неоправданно сложным. 

%The crash dump is going to be named \filename{erl\_crash.dump} and be located wherever the Erlang process was running by default. This behaviour (and the file name) can be overridden by specifying the \command{ERL\_CRASH\_DUMP} environment variable\footnote{Heroku's Routing and Telemetry teams use the \otpapp{\href{https://github.com/heroku/heroku\_crashdumps}{heroku\_crashdumps}} app to set the path and name of the crash dumps. It can be added to a project to name the dumps by boot time and put them in a pre-set location}.
Аварийный дамп получает имя \filename{erl\_crash.dump} и находится в директории, в которой по умолчанию исполнялся Erlang-процесс. Это поведение (и имя файла) могут быть изменены с помощью указания переменной окружения \command{ERL\_CRASH\_DUMP}\footnote{Команды маршрутизации и телеметрии в компании Heroku используют приложение \otpapp{\href{https://github.com/heroku/heroku\_crashdumps}{heroku\_crashdumps}}, чтобы устанавливать путь и имя файлов дампов. Его можно добавить в проект, чтобы давать имя дампам во время запуска и помещать их в заранее заданную директорию.}.


%\section{General View}
\section{Общий вид}
\label{sec:crashdump-general-view}

%Reading the crash dump will be useful to figure out possible reasons for a node to die \emph{a posteriori}. One way to get a quick look at things is to use recon's \app{erl\_crashdump\_analyzer.sh}\footnote{\href{https://github.com/ferd/recon/blob/master/script/erl\_crashdump\_analyzer.sh}{https://github.com/ferd/recon/blob/master/script/erl\_crashdump\_analyzer.sh}} and run it on a crash dump:
Чтение аварийного дампа пригодится для того, чтобы \emph{посмертно} выяснить возможные причины, почему узел погиб. Одним из способов быстро посмотреть на эти данные является скрипт \app{erl\_crashdump\_analyzer.sh} в составе recon\footnote{\href{https://github.com/ferd/recon/blob/master/script/erl\_crashdump\_analyzer.sh}{https://github.com/ferd/recon/blob/master/script/erl\_crashdump\_analyzer.sh}} --- просто выполните его и укажите параметром файл аварийного дампа.

%% Show debugging here with output
\begin{VerbatimRaw}
$ ./recon/script/erl_crashdump_analyzer.sh erl_crash.dump
analyzing erl_crash.dump, generated on:  Thu Apr 17 18:34:53 2014

Slogan: eheap_alloc: Cannot allocate 2733560184 bytes of memory
(of type "old_heap").

Memory:
===
  processes: 2912 Mb
  processes_used: 2912 Mb
  system: 8167 Mb
  atom: 0 Mb
  atom_used: 0 Mb
  binary: 3243 Mb
  code: 11 Mb
  ets: 4755 Mb
  ---
  total: 11079 Mb

Different message queue lengths (5 largest different):
===
      1 5010932
      2 159
      5 158
     49 157
      4 156

Error logger queue length:
===
0

File descriptors open:
===
  UDP:  0
  TCP:  19951
  Files:  2
  ---
  Total:  19953

Number of processes:
===
36496

Processes Heap+Stack memory sizes (words) used in the VM (5 largest
different):
===
      1 284745853
      1 5157867
      1 4298223
      2 196650
     12 121536

Processes OldHeap memory sizes (words) used in the VM (5 largest
different):
===
      3 318187
      9 196650
     14 121536
     64 75113
     15 46422

Process States when crashing (sum):
===
      1 Garbing
     74 Scheduled
  36421 Waiting
\end{VerbatimRaw}

%This data dump won't point out a problem directly to your face, but will be a good clue as to where to look. For example, the node here ran out of memory and had 11079 Mb out of 15 Gb used (I know this because that's the max instance size we were using!) This can be a symptom of:
Этот дамп не преподносит проблему прямо вам под нос, но даст хорошие подсказки, куда смотреть. Например, у узла здесь закончилась память и было использовано 11.079 мегабайт из 15 гигабайт (я знаю максимальную цифру, потому что это размер виртуальной машины, которую мы использовали в этот день!). Это может быть симптомом следующих проблем:

\begin{itemize*}
%	\item memory fragmentation;
	\item фрагментации памяти;
	
	\item memory leaks in C code or drivers;
	\item утечек памяти в С-коде или драйверах;
	
%	\item lots of memory that got to be garbage-collected before generating the crash dump\footnote{Notably here is reference-counted binary memory, which sits in a global heap, but ends up being garbage-collected before generating the crash dump. The binary memory can therefore be underreported. See Chapter \NamedRef{chap:memory-leaks} for more details}.
	\item очень много блоков памяти, которую следовало бы собрать сборщику мусора перед тем, как создавать аварийный дамп\footnote{Особенно бросается в глаза память больших двоичных данных, которая находится в глобальной куче, но ей достаётся проход сборщика мусора перед созданием дампа. Расход памяти на двоичные данные, таким образом, в этом отчёте может быть занижен. Смотрите главу \NamedRef{chap:memory-leaks} для подробного описания.}.
\end{itemize*}

%More generally, look for anything surprising for memory there. Correlate it with the number of processes and the size of mailboxes. One may explain the other. 
В общем, ищите среди цифр памяти что угодно, привлекающее внимание. Сравнивайте это с числом процессов и размерами почтовых ящиков. Одно может объяснить причины второго.

%In this particular dump, one process had 5 million messages in its mailbox. That's telling. Either it doesn't match on all it can get, or it is getting overloaded. There are also dozens of processes with hundreds of messages queued up — this can point towards overload or contention. It's hard to have general advice for your generic crash dump, but there still are a few pointers to help figure things out.
В этом конкретном дампе один из процессов имел 5 миллионов сообщений в почтовом ящике. Это всё объясняет. Он не выбирал все сообщения, а лишь некоторые, его интересующие, или просто был перегружен. Также имеются десятки процессов с сотнями сообщений в очереди --- это может указывать на перегрузку либо борьбу за ресурсы. Трудно дать общий совет для дампа, который может иметься в вашем случае, но всё-же имеются некоторые подсказки, с которых можно начать выяснять причины проблем.


%\section{Full Mailboxes}
\section{Полные почтовые ящики}
\label{sec:crash-full-mailboxes}

%For loaded mailboxes, looking at large counters is the best way to do it. If there is one large mailbox, go investigate the process in the crash dump. Figure out if it's happening because it's not matching on some message, or overload. If you have a similar node running, you can log on it and go inspect it. If you find out many mailboxes are loaded, you may want to use recon's \app{queue\_fun.awk} to figure out what function they're running at the time of the crash:
Для нагруженных почтовых ящиков, взгляд на большие счётчики является лучшим способом выяснить наличие проблемы. Если имеется один большой почтовый ящик --- расследуйте процесс в аварийном дампе. Выясните, происходит ли это потому, что процесс не выбирает какой-то вид сообщений, или перегружен. Если у вас окажется множество нагруженных почтовых ящиков, вы можете захотеть воспользоваться скриптом \app{queue\_fun.awk} в пакете recon, чтобы выяснить, какую функцию все они выполняли на момент аварийного завершения:

\begin{VerbatimText}
$ awk -v threshold=10000 -f queue_fun.awk /path/to/erl_crash.dump 
MESSAGE QUEUE LENGTH: CURRENT FUNCTION
======================================
10641: io:wait_io_mon_reply/2
12646: io:wait_io_mon_reply/2
32991: io:wait_io_mon_reply/2
2183837: io:wait_io_mon_reply/2
730790: io:wait_io_mon_reply/2
80194: io:wait_io_mon_reply/2
...
\end{VerbatimText}

%This one will run over the crash dump and output all of the functions scheduled to run for processes with at least 10000 messages in their mailbox. In the case of this run, the script showed that the entire node was locking up waiting on IO for \function{io:format/2} calls, for example.
Этот скрипт пробежит по содержимому аварийного дампа и выведет все функции, которые были подготовлены планировщиком к запуску, и имели не менее 10.000 сообщений в своих почтовых ящиках. В моём случае, например, сценарий показал, что весь узел был заблокирован в ожидании ввода-вывода для печати посредством \function{io:format/2}.


%\section{Too Many (or too few) Processes}
\section{Слишком много (или мало) процессов}

%The process count is mostly useful when you know your node's usual average count\footnote{See subsection \NamedRef{subsec:global-procs} for details}, in order to figure if it's abnormal or not.
Количество процессов пригодится, если вы знаете обычную повседневную цифру количества для вашего узла\footnote{Смотрите подсекцию \NamedRef{subsec:global-procs} для подробностей}, чтобы понять, когда начнёт происходить что-то необычное.

%A count that is higher than normal may reveal a specific leak or overload, depending on applications.
Количество больше обычного может указывать на некоторую утечку или перегрузку, в зависимости от вашего приложения.

%If the process count is extremely low compared to usual, see if the node terminated with a slogan like:
Если количество процессов очень мало, в сравнении с обычным количеством, проверьте нет ли в журналах сообщений вроде следующего:

\begin{Verbatim}
Kernel pid terminated (application_controller)
  ({application_terminated, <ИмяПриложения>, shutdown})
\end{Verbatim}

%In such a case, the issue is that a specific application (\expression{<AppName>}) has reached its maximal restart frequency within its supervisors, and that prompted the node to shut down. Error logs that led to the cascading failure should be combed over to figure things out.
В таком случае проблемой является то, что некоторое приложение (\expression{<ИмяПриложения>}) достигло максимальной частоты перезапусков своих наблюдателей и это потребовало аварийной остановки всего узла. Следует вычитать журналы в поисках ошибок, приведших к каскадному отказу.


%\section{Too Many Ports}
\section{Слишком много портов}

%Similarly to the process count, the port count is simple and mostly useful when you know your usual values\footnote{See subsection \NamedRef{subsec:global-ports} for details}.
Подобно количеству процессов, количество портов --- это простая и в целом полезная характеристика, когда вы знаете обычные значения для вашей системы\footnote{Смотрите подсекцию \NamedRef{subsec:global-ports} для подробностей}.

%A high count may be the result of overload, Denial of Service attacks, or plain old resource leaks. Looking at the type of port leaked (TCP, UDP, or files) can also help reveal if there was contention on specific resources, or if the code using them is just wrong.
Большое количество может быть последствием перегрузки, DoS атак или старых добрых утечек ресурсов. Глядя на тип утекших портов (TCP, UDP или файлы) можно выяснить, была ли борьба за какой-то ресурс или код, с ними работающий, содержит ошибку.


%\section{Can't Allocate Memory}
\section{Невозможно выделить память}

%These are by far the most common types of crashes you are likely to see. There's so much to cover, that Chapter \NamedRef{chap:memory-leaks} is dedicated to understanding them and doing the required debugging on live systems.
Эти ошибки встречаются намного чаще других. Причин такой остановки настолько много, что я посвятил их пониманию и выполнению необходимой отладки на живых системах целую главу \NamedRef{chap:memory-leaks}.

%In any case, the crash dump will help figure out what the problem was after the fact. The process mailboxes and individual heaps are usually good indicators of issues. If you're running out of memory without any mailbox being outrageously large, look at the processes heap and stack sizes as returned by the recon script.
В любом случае, аварийный дамп поможет выяснить, что случилось, по факту когда программа завершилась. Почтовые ящики процессов --- это отдельные кучи и обычно являются хорошими индикаторами возникновения проблем. Если у вас заканчивается память при том, что ни один почтовый ящик не вырос до заоблачных высот, посмотрите на кучи процессов и размеры стеков, которые возвращает скрипт recon.

%In case of large outliers at the top, you know some restricted set of processes may be eating up most of your node's memory. In case they're all more or less equal, see if the amount of memory reported sounds like a lot.
Если у вас имеются резко выделяющиеся числа в отчёте, то вы сможете составить некоторый ограниченный набор процессов, которые подозреваются в перерасходе памяти. В случае же если все они более менее равны, посмотрите на сообщаемое количество использованной памяти, вдруг она покажет большие числа.

%If it looks more or less reasonable, head towards the "Memory" section of the dump and check if a type (ETS or Binary, for example) seems to be fairly large. They may point towards resource leaks you hadn't expected.
Если всё выглядит более-менее разумно, перейдите в секцию с отчётом о памяти (<<Memory>>) и проверьте, если один из типов памяти (ETS или двоичная куча, например) выглядит неоправданно большим. Они могут указывать на утечки ресурсов, которых вы не ожидали.


\section{Упражнения}

\subsection*{\ReviewTitle{}}

\begin{enumerate}
%	\item How can you choose where a crash dump will be generated?
	\item Как можно выбрать директорию, в которой будет создан аварийный дамп?
%	\item What are common avenues to explore if the crash dump shows that the node ran out of memory?
	\item Назовите что следует искать в дампе, когда причиной смерти узла был назван недостаток памяти?
%	\item What should you look for if the process count is suspiciously low?
	\item Что следует искать, если количество процессов необычно мало?
%	\item If you find the node died with a process having a lot of memory, what could you do to find out which one it was?
	\item Как вы узнали, причиной смерти узла стало то, что один из процессов начал использовать слишком много памяти. Что можно сделать, чтобы найти этот процесс?
\end{enumerate}

\subsection*{\HandsOnTitle{}}

%Using the analysis of a crash dump in Section \NamedRef{sec:crashdump-general-view}:
Используя анализ аварийного дампа в секции \NamedRef{sec:crashdump-general-view}:

\begin{enumerate}
%	\item What are specific outliers that could point to an issue?
	\item Какие числа являются необычно выделяющимися и могут указывать на возможные причины?
%	\item Does it look like repeated errors are the issue? If not, what could it be? 
	\item Могли ли повторяющиеся ошибки стать проблемой? Если нет, что же тогда могло?
\end{enumerate}

%%%
%%%
%%%

%%% Cognitive
%%
%% These should be done after reading the standard documentation on crash dumps referenced by the text.
%%
%% Knowledge: recall facts, terms, basic concepts
%% 
%% - What's a crash dump?
%% - How can you choose where a crash dump will be generated?
%% - What are common avenues to explore if the crash dump shows that the node ran out of memory?
%% - What should you look for if the process count is suspiciously low?
%%
%% Comprehension: organizing, comparing, translating, interpreting, giving descriptions, and stating the main ideas
%%
%% - In the crash dump analysis shown in <7.1 General View>, what are specific outliers that could point to an issue?
%%
%% Application: Solve problems in new situations by applying acquired knowledge, facts, techniques and rules in a different way
%%
%% - In the crash dump analysis shown before, does it look like repeated errors are the issue? If not, what could it be?
%% - What can you explore if you find many processes have large mailboxes as a backlog?
%%
%% Analysis: break down info, make inferences, find evidence
%%
%% - What could be used in a crash dump to point at repeated failures before the node died?
%% - 
%%
%% Synthesis: Compile information together in a different way by combining elements in a new pattern or proposing alternative solutions
%%
%% - If you find the node died with a process having a lot of memory, what could you do to find out which one it was?
%%
%% Evaluation: Present and defend opinions by making judgments about information
%%
%% - Do you think there's value to crash dumps alone? What would be the most efficient use of them in a real world product's operational cycle?
%% 
